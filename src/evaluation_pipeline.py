#!/usr/bin/env python3.10
"""
Pipeline to evaluate predictions generated by the competition_pipeline.py. Generates our three ensembles from the raw
predictions and compares them to the benchmarks across all prediction windows.
"""

from collections.abc import Callable, Iterable
from functools import partial
import os
import warnings

from joblib import Parallel, delayed
import numpy as np
import pandas as pd
import xarray as xr

from src.utils.conversion import get_date
from src.utils.data_prep import read_prio_actuals, read_benchmarks, read_predictions
from src.utils.evaluation import create_metrics_df


def bootstrap_metrics(observed:xr.DataArray, predictions_dict: dict[str, xr.DataArray],
                      sample_size: int = 1000, bootstrap_samples: int = 1000,
                      parallel_kwargs: dict = {'n_jobs': 8, 'verbose': 1}) -> pd.DataFrame:
    """
    Function to bootstraps metrics based on random spatial sampling of priogrid cells. Creates sample, selects data and
    calculates metrics. Bootstrapping implements parallelization via joblib.Parallel.

    Args:
        observed: xr.DataArray with observations.
        predictions_dict: dictionary of format {key: xr.DataArray} with any predictions for which to bootstrap metrics.
        sample_size: number of grid cells in a sample.
        bootstrap_samples: number of boostrap samples to draw and evaluate.
        parallel_kwargs: dict with kwargs to pass to joblib.Parallel.

    Returns:
        Dataframe with metrics as index and predictions_dict keys as columns with each dataframe entry containing the
        values for all bootstrap samples for the given combination.
    """
    def bootstrap_parallel(sample_pgids: Iterable[int|float]) -> pd.DataFrame:
        """ helper function which runs in parallel and returns the metrics for all included predictions in a sample """
        observed_sample = observed.loc[:, sample_pgids]  # pay attention to the 1 fewer dimensions!
        predictions_dict_sample = {model: preds.loc[:, sample_pgids, :] for model, preds in predictions_dict.items()}
        df_metrics_sample = create_metrics_df(observed_sample, predictions_dict_sample)
        return df_metrics_sample

    rng = np.random.default_rng(1465322168798791321354823142454321779949112123547241)
    pgids = next(iter(predictions_dict.values())).priogrid_gid.values
    metrics_list = Parallel(**parallel_kwargs)(
        delayed(bootstrap_parallel)(rng.choice(pgids, size=sample_size, replace=True)) for i in range(bootstrap_samples)
    )

    # create unified dataframe
    metrics = metrics_list[0].index
    models = metrics_list[0].columns
    df_bootstrap_scores = pd.DataFrame(index=metrics, columns=models)
    for metric in metrics:
        for model in models:
            df_bootstrap_scores.at[metric, model] = np.array([df.at[metric, model] for df in metrics_list])

    return df_bootstrap_scores


def evaluation_pipeline(year: int, load_data_func: Callable[[int], tuple[xr.DataArray, dict[str, xr.DataArray]]],
                        fp_out: str = None, save: bool = True, bootstrap: bool = True,
                        bootstrap_kwargs: dict = None, return_results: bool = False,
                        regenerate_metrics: bool = True) -> pd.DataFrame|tuple[pd.DataFrame]|None:
    """
    Pipeline for evaluation against the various prediction competition and some additional metrics. Generates dataframes
    with the results and can store or return them. Optionally also spatially bootstraps metrics to be able to generate
    confidence intervals.

    Args:
        year: yearly prediction window to run the pipeline for.
        load_data_func: function to load the desired data for the window. It should take the argument year and return
            a tuple of (observed, predictions[dict with string keys and corresponding data]), with all data as
            xr.DataArrays.
        fp_out: (optional) filepath to store evaluation results if save=True.
        save: (optional) whether to save the resulting dataframes.
        bootstrap: (optional) whether to perform bootstrapping to calculate confidence intervals for the metrics.
        bootstrap_kwargs: (optional) dict with kwargs to be passed to bootstrap_metrics if bootstrap=True.
        return_results: (optional) whether to return the results generated.
        regenerate_metrics: (optional) whether to regenerate metrics if saved results already exist or load these
            results instead.

    Returns:
        depending on return_results and bootstrap it will return either None, the evaluation results or a tuple
        (evaluation results, bootstrapped evaluation results), with results being pd.Dataframes.
    """
    if save and fp_out is None:
        fp_out = os.getcwd()
        warnings.warn(f'No output filepath ("fp_out") defined, will save results to current directory: {os.getcwd()}')
    if bootstrap_kwargs is None:
        bootstrap_kwargs = {}

    print(f'##### window {year} #####')
    fp_out_metrics = os.path.join(fp_out, 'metrics')
    fp_metrics_window = os.path.join(fp_out_metrics, f'metrics_benchmarks_full_{year}.csv')
    if os.path.exists(fp_metrics_window) and not regenerate_metrics:
        print(f'Read existing prediction and benchmark metrics...')
        df_metrics = pd.read_csv(fp_metrics_window, index_col=0)
        data_loaded = False
    else:
        print('Loading predictions and creating benchmarks...')
        observed, predictions_dict = load_data_func(year)
        data_loaded = True
        print(f'Calculating prediction and benchmark metrics...')
        df_metrics = create_metrics_df(observed, predictions_dict)
        if save:
            if not os.path.exists(fp_out_metrics):
                os.makedirs(fp_out_metrics)
            df_metrics.to_csv(fp_metrics_window)

    if bootstrap:
        print('Bootstrapping metrics...')
        fp_out_bootstrap = os.path.join(fp_out, 'bootstrap')
        fp_bootstrap_window = os.path.join(fp_out, 'bootstrap', f'metrics_bootstrap_{year}.parquet')
        if os.path.exists(fp_bootstrap_window) and not regenerate_metrics:
            print('reading existing bootstrap scores...')
            df_bootstrap_scores = pd.read_parquet(fp_bootstrap_window)
        else:
            if not data_loaded:
                observed, predictions_dict = load_data_func(year)
            df_bootstrap_scores = bootstrap_metrics(observed, predictions_dict, **bootstrap_kwargs)
            if save:
                if not os.path.exists(fp_out_bootstrap):
                    os.makedirs(fp_out_bootstrap)
                df_bootstrap_scores.to_parquet(fp_bootstrap_window)
        if return_results:
            return df_metrics, df_bootstrap_scores
    if return_results:
        return df_metrics
    else:
        return


if __name__ == "__main__":
    fp_views = '../views_data/'
    fp_raw_predictions = 'raw_predictions_hdbscan'
    fp_out = f'evaluation/'
    test_windows = [2018, 2019, 2020, 2021, 2022, 2023] # we can of course only evaluate up to 2023

    # to be flexible in terms of which benchmarks to include etc. we define the function loading the data here
    def load_evaluation_data(year: int, fp_views: str):
        print('load predictions...')
        observed = read_prio_actuals(fp_views, year=year)

        models = ['global', 'local', 'global-local']
        predictions_dict = {}
        for model in models:
            predictions_dict[model] = read_predictions(f'unibw_trees_{model}', year)
        print(get_date(predictions_dict['global'].month_id.values.min()), get_date(predictions_dict['global'].month_id.values.max()))
        print('load benchmarks...')
        benchmarks = ['conflictology_n', 'last', 'zero', 'conflictology', 'boot_240']
        views_benchmarks = {bench: read_benchmarks(fp_views, year, bench) for bench in benchmarks}

        predictions_dict = {**predictions_dict, **views_benchmarks}
        print('data loaded!')
        return observed, predictions_dict

    load_data_func = partial(load_evaluation_data, fp_views=fp_views)


    for year in test_windows:
        print('mean fatalities:', float(read_prio_actuals(fp_views, year).mean()))
        evaluation_pipeline(year, load_data_func, fp_out, save=True, bootstrap=False, regenerate_metrics=True,
                            return_results=False)

